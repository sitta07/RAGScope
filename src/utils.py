import streamlit as st
import os
import time
import pandas as pd
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# Constants
DB_PATH = "./processed_data/chroma_db"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

@st.cache_resource
def load_vector_db():
    print("üîÑ Loading Vector DB...")
    embedding_function = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    vector_db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embedding_function,
        collection_name="rag_demo"
    )
    return vector_db

@st.cache_resource
def get_all_documents_metadata(_vector_db):
    """
    ‡∏î‡∏∂‡∏á Metadata ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (x, y, source) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÑ‡∏õ plot ‡∏•‡∏á‡∏Å‡∏£‡∏≤‡∏ü
    ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á Vector (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM)
    """
    data = _vector_db.get(include=["metadatas"])
    df = pd.DataFrame(data['metadatas'])
    return df

def get_llm(api_key):
    """Initialize Groq LLM (Llama3-70b)"""
    if not api_key:
        return None
    
    return ChatGroq(
        groq_api_key=api_key,
        model_name="llama3-70b-8192",
        temperature=0.0
    )

def perform_rag(query, vector_db, llm, strategy="basic"):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô RAG ‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Strategy ‡πÑ‡∏î‡πâ
    - Basic: Similarity Search ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤
    - Advanced: MMR (Maximal Marginal Relevance) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    """
    start_time = time.time()
    
    # 1. Retrieval Strategy
    if strategy == "advanced":
        # MMR ‡∏ä‡πà‡∏ß‡∏¢‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏∏‡∏Å‡∏ï‡∏±‡∏ß
        retriever = vector_db.as_retriever(
            search_type="mmr", 
            search_kwargs={"k": 4, "fetch_k": 10, "lambda_mult": 0.5}
        )
    else:
        # Basic Similarity Search
        retriever = vector_db.as_retriever(search_kwargs={"k": 4})
        
    # 2. Get Documents
    docs = retriever.invoke(query)
    
    # 3. Generation (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ LLM)
    answer = "N/A (No API Key)"
    if llm:
        # Simple Prompt
        template = """Answer the question based only on the context below. 
        If you don't know, say "I don't know". Keep it professional.
        
        Context: {context}
        
        Question: {question}
        """
        prompt = PromptTemplate.from_template(template)
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
        
        # Run Chain
        result = chain.invoke({"query": query})
        answer = result['result']
        docs = result['source_documents'] # Update docs from chain result
        
    latency = time.time() - start_time
    return answer, docs, latency