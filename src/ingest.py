import os
import shutil
import numpy as np
import pandas as pd
import umap
from tqdm import tqdm
from uuid import uuid4

# LangChain & Chroma Stack
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# --- Configuration ---
# üî• ‡∏õ‡∏£‡∏±‡∏ö Path ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå Harry Potter ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á
DATA_PATH = "./data"
DB_PATH = "./processed_data/chroma_db"
COLLECTION_NAME = "harry_potter_lore" # ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏≠‡∏∑‡πà‡∏ô
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# 1. Setup Embeddings 
print(f"üîÆ Initializing Embedding Model ({EMBEDDING_MODEL_NAME})...")
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

def load_documents():
    """Load documents from data folder"""
    if not os.path.exists(DATA_PATH):
        print(f"‚ùå Error: Folder '{DATA_PATH}' not found. Please create it and add .txt files.")
        return []

    print(f"üìÇ Loading documents from {DATA_PATH}...")
    
    loaders = {
        ".txt": TextLoader,
        ".pdf": PyPDFLoader
    }
    
    docs = []
    # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    for filename in os.listdir(DATA_PATH):
        ext = os.path.splitext(filename)[1]
        if ext in loaders:
            try:
                loader = loaders[ext](os.path.join(DATA_PATH, filename))
                loaded_docs = loader.load()
                # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô metadata ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ filter/‡πÇ‡∏ä‡∏ß‡πå‡πÉ‡∏ô UI
                for doc in loaded_docs:
                    doc.metadata["source_doc"] = filename
                docs.extend(loaded_docs)
                print(f"   - ‚úÖ Loaded: {filename}")
            except Exception as e:
                print(f"   - ‚ö†Ô∏è Failed to load {filename}: {e}")
            
    return docs

def process_chunks(documents):
    print("‚öîÔ∏è Splitting documents into chunks...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,      # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Lore
        chunk_overlap=100,   # ‡∏Å‡∏±‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≤‡∏î‡∏ï‡∏≠‡∏ô
        separators=["\n\n", "\n", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"   - Generated {len(chunks)} magical chunks.")
    return chunks

def compute_umap_coords(embeddings):
    """
    ‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥ Vector (384) -> 2D (x,y) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Plot Graph
    """
    print("üó∫Ô∏è Computing UMAP 2D projections...")
    
    # Safety Check: UMAP ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ n_neighbors ‡∏à‡∏∞ error
    n_samples = len(embeddings)
    n_neighbors = 15
    
    if n_samples <= 15:
        n_neighbors = max(2, n_samples - 1)
        print(f"   - ‚ö†Ô∏è Data is small ({n_samples}), adjusting n_neighbors to {n_neighbors}")

    reducer = umap.UMAP(
        n_neighbors=n_neighbors,
        n_components=2,
        metric='cosine',
        random_state=42 # Fix seed ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≤‡∏ü‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    )
    
    embeddings_array = np.array(embeddings)
    umap_coords = reducer.fit_transform(embeddings_array)
    
    return umap_coords

def main():
    # Clear DB ‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Collection ‡∏ô‡∏µ‡πâ (‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡πÑ‡∏î‡πâ) ‡πÅ‡∏ï‡πà Chroma ‡πÅ‡∏ö‡∏ö Local ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤
    # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏•‡∏ö DB_PATH ‡∏ó‡∏¥‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠ Clean Start (‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Collection ‡∏≠‡∏∑‡πà‡∏ô‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà)
    # ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡πÉ‡∏ô Demo ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ folder ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÄ‡∏£‡∏≤
    # if os.path.exists(DB_PATH):
    #     shutil.rmtree(DB_PATH)
    #     print("üßπ Cleared old database.")

    # 1. Load Data
    raw_docs = load_documents()
    if not raw_docs:
        return

    # 2. Chunking
    chunks = process_chunks(raw_docs)
    if not chunks:
        print("‚ùå No chunks created.")
        return
    
    # 3. Generate Embeddings for ALL chunks
    print("üß† Generating Embeddings (Focusing logic)...")
    chunk_texts = [doc.page_content for doc in chunks]
    embeddings = embedding_model.embed_documents(chunk_texts)
    
    # 4. Pre-compute Visualization Coordinates (UMAP)
    umap_coords = compute_umap_coords(embeddings)
    
    # 5. Save to ChromaDB with Metadata
    print(f"üíæ Saving to Vector DB (Collection: {COLLECTION_NAME})...")
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Client
    vector_db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embedding_model,
        collection_name=COLLECTION_NAME
    )
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Data ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Upsert
    ids = [str(uuid4()) for _ in range(len(chunks))]
    metadatas = []
    
    for i, chunk in enumerate(chunks):
        meta = chunk.metadata.copy()
        # Add UMAP coordinates to metadata (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö)
        meta["umap_x"] = float(umap_coords[i][0])
        meta["umap_y"] = float(umap_coords[i][1])
        metadatas.append(meta)

    # Batch Upsert (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)
    BATCH_SIZE = 100
    total_batches = (len(chunks) + BATCH_SIZE - 1) // BATCH_SIZE
    
    for i in tqdm(range(total_batches), desc="Upserting chunks"):
        start = i * BATCH_SIZE
        end = start + BATCH_SIZE
        
        batch_texts = chunk_texts[start:end]
        batch_metadatas = metadatas[start:end]
        batch_ids = ids[start:end]
        
        vector_db.add_texts(
            texts=batch_texts,
            metadatas=batch_metadatas,
            ids=batch_ids
        )
        
    print(f"‚úÖ Mischief Managed! Ingested {len(chunks)} chunks into {DB_PATH}")
    print("‚û°Ô∏è Ready to run app.py")

if __name__ == "__main__":
    main()